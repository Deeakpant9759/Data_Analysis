{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70063f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process, fuzz\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fbfcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emd = pd.read_excel(\"c:\\\\Users\\\\MICILMEDS\\\\Downloads\\\\UPDATED STOCK LIST .xlsx\")\n",
    "df_mcl  = pd.read_excel(\"c:\\\\Users\\\\MICILMEDS\\\\Downloads\\\\20250925031502.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5428e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d40a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, time\n",
    "from rapidfuzz import process, fuzz\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming df_emd and df_mcl are already loaded\n",
    "# df_emd = pd.read_csv('your_emd_file.csv')\n",
    "# df_mcl = pd.read_csv('your_mcl_file.csv')\n",
    "\n",
    "THRESHOLD = 90\n",
    "BLOCK_LEN = 2  # first N chars as blocking key\n",
    "\n",
    "# --- STEP 1: CLEAN TEXT ---\n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r'[^a-z0-9 ]', ' ', s)  # Fixed: added space after comma\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "# Apply cleaning - make sure column names exist\n",
    "df_emd['clean_name'] = df_emd['Product Name'].map(clean_text)\n",
    "df_mcl['clean_name'] = df_mcl['Product Name'].map(clean_text)\n",
    "\n",
    "# Create blocking keys\n",
    "df_emd['block'] = df_emd['clean_name'].str[:BLOCK_LEN]\n",
    "df_mcl['block'] = df_mcl['clean_name'].str[:BLOCK_LEN]\n",
    "\n",
    "# --- STEP 2: BLOCK-WISE MATCHING ---\n",
    "start = time.time()\n",
    "results = []\n",
    "\n",
    "# Only keep blocks present in both datasets\n",
    "blocks = sorted(set(df_emd['block']) & set(df_mcl['block']))\n",
    "print(f\"Processing {len(blocks)} blocks...\")\n",
    "\n",
    "for blk in tqdm(blocks, desc=\"Matching blocks\"):\n",
    "    emd_block = df_emd[df_emd['block'] == blk].reset_index(drop=True)\n",
    "    mcl_block = df_mcl[df_mcl['block'] == blk].reset_index(drop=True)\n",
    "    \n",
    "    # Skip empty blocks\n",
    "    if len(emd_block) == 0 or len(mcl_block) == 0:\n",
    "        continue\n",
    "    \n",
    "    choices = mcl_block['clean_name'].tolist()\n",
    "    \n",
    "    # Better approach: Create mapping without using index\n",
    "    # This handles duplicates by keeping all occurrences\n",
    "    mcl_lookup = {}\n",
    "    for _, mcl_row in mcl_block.iterrows():\n",
    "        clean_name = mcl_row['clean_name']\n",
    "        if clean_name not in mcl_lookup:\n",
    "            mcl_lookup[clean_name] = {\n",
    "                'Product Name': mcl_row['Product Name'],\n",
    "                'Product Code': mcl_row['Product Code']\n",
    "            }\n",
    "    \n",
    "    for _, row in emd_block.iterrows():\n",
    "        try:\n",
    "            match = process.extractOne(\n",
    "                row['clean_name'],\n",
    "                choices,\n",
    "                scorer=fuzz.token_sort_ratio,\n",
    "                score_cutoff=THRESHOLD\n",
    "            )\n",
    "            if match:\n",
    "                matched_name, score, _ = match  # unpack 3 values\n",
    "                matched_info = mcl_lookup[matched_name]\n",
    "                \n",
    "                results.append({\n",
    "                    'Product Name': row['Product Name'],\n",
    "                    'matched_name': matched_info['Product Name'],\n",
    "                    'match_score': score\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row: {row['Product Name']} - {e}\")\n",
    "            continue\n",
    "\n",
    "# Create results DataFrame\n",
    "matches_df = pd.DataFrame(results)\n",
    "print(f\"✅ Completed in {time.time()-start:.2f} seconds for {len(matches_df)} matches\")\n",
    "\n",
    "# Optional: Display sample results\n",
    "if len(matches_df) > 0:\n",
    "    print(\"\\nSample matches:\")\n",
    "    print(matches_df.head())\n",
    "    print(f\"\\nMatch score distribution:\")\n",
    "    print(matches_df['match_score'].describe())\n",
    "else:\n",
    "    print(\"No matches found. Consider lowering the threshold or checking your data.\")\n",
    "\n",
    "# Optional: Save results\n",
    "# matches_df.to_csv('fuzzy_matches.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c4d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e0666",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b58f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598366f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df['product_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062bb62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df['matched_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee72f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"c:\\\\Users\\\\MICILMEDS\\\\Documents\\\\Medi_final\\\\correct_data\\\\Category\\\\matched_Products.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d4be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a56dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df3350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Merged = pd.merge(df, matches_df, how='inner', left_on='product_code', right_on='product_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeba0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Merged.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe0b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Merged[['product_code','image_path','matched_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# --- Assuming df_Merged is already loaded ---\n",
    "# For testing, take only first 50 rows\n",
    "\n",
    "# Base paths\n",
    "base_path = r\"C:\\Users\\MICILMEDS\\Downloads\\EMed_Image\\html\\uploads\\product\\main\"\n",
    "destination = r\"C:\\Users\\MICILMEDS\\Documents\\Medi_final\\correct_data\\Category\\Drive_images\"\n",
    "\n",
    "# Create destination folder if not exists\n",
    "os.makedirs(destination, exist_ok=True)\n",
    "\n",
    "for idx, row in df_Merged.iterrows():\n",
    "    image_path = row['image_path']          # e.g. medicine/1_2.jpg\n",
    "    matched_code = row['matched_code']      # e.g. MIC162999\n",
    "\n",
    "    # Full source path\n",
    "    src_path = os.path.join(base_path, image_path)\n",
    "\n",
    "    # Extract suffix after the first underscore\n",
    "    file_name = os.path.basename(image_path)   # e.g. 1_2.jpg\n",
    "    suffix = file_name.split(\"_\", 1)[-1]       # e.g. 2.jpg\n",
    "\n",
    "    # Create new name\n",
    "    new_name = f\"{matched_code}_{suffix}\"\n",
    "    dest_path = os.path.join(destination, new_name)\n",
    "\n",
    "    # Copy the image\n",
    "    try:\n",
    "        shutil.copy2(src_path, dest_path)\n",
    "        print(f\"✅ Copied: {src_path} --> {dest_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Missing file: {src_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error copying {src_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3719dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont, UnidentifiedImageError\n",
    "from pathlib import Path\n",
    "\n",
    "folder = Path(r\"C:\\Users\\MICILMEDS\\Documents\\Medi_final\\correct_data\\Category\\Drive_images\")\n",
    "\n",
    "watermark_text = \"MICYLMEDS\"\n",
    "font_size = 40\n",
    "opacity = 60  # 0-255\n",
    "angle = 30\n",
    "\n",
    "# Load font\n",
    "try:\n",
    "    font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "except:\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "for file in folder.glob(\"*.*\"):\n",
    "    if file.suffix.lower() not in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with Image.open(file).convert(\"RGBA\") as base:\n",
    "\n",
    "            # Resize image; if fails, slightly adjust size\n",
    "            try:\n",
    "                base = base.resize((1024, 871))\n",
    "            except Exception:\n",
    "                # Fallback: scale proportionally if exact resize fails\n",
    "                base.thumbnail((1024, 871))\n",
    "\n",
    "            # Create watermark layer\n",
    "            watermark_layer = Image.new(\"RGBA\", base.size, (0, 0, 0, 0))\n",
    "            draw = ImageDraw.Draw(watermark_layer)\n",
    "\n",
    "            # Get text size\n",
    "            bbox = draw.textbbox((0, 0), watermark_text, font=font)\n",
    "            text_width = bbox[2] - bbox[0]\n",
    "            text_height = bbox[3] - bbox[1]\n",
    "\n",
    "            # Repeat watermark across image\n",
    "            x_repeat = range(0, base.width, text_width + 100)\n",
    "            y_repeat = range(0, base.height, text_height + 100)\n",
    "\n",
    "            for x in x_repeat:\n",
    "                for y in y_repeat:\n",
    "                    draw.text((x, y), watermark_text, font=font, fill=(255, 255, 255, opacity))\n",
    "\n",
    "            # Rotate watermark around center\n",
    "            watermark_layer = watermark_layer.rotate(angle, expand=True)\n",
    "            cx, cy = watermark_layer.width // 2, watermark_layer.height // 2\n",
    "            bx, by = base.width // 2, base.height // 2\n",
    "            watermark_layer = watermark_layer.crop((cx - bx, cy - by, cx - bx + base.width, cy - by + base.height))\n",
    "\n",
    "            # Merge watermark with base image\n",
    "            final = Image.alpha_composite(base, watermark_layer)\n",
    "\n",
    "            # Save final image\n",
    "            final.convert(\"RGB\").save(file, quality=95)\n",
    "            print(f\"✅ Processed: {file.name}\")\n",
    "\n",
    "    except UnidentifiedImageError:\n",
    "        print(f\"⚠️ Skipped (not a valid image): {file.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipped ({e}): {file.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import math\n",
    "from tqdm import tqdm  # pip install tqdm\n",
    "\n",
    "# Source folder\n",
    "source_folder = Path(r\"C:\\Users\\MICILMEDS\\Documents\\Medi_final\\correct_data\\Category\\Drive_images\")\n",
    "if not source_folder.exists():\n",
    "    raise FileNotFoundError(f\"Source folder does not exist: {source_folder}\")\n",
    "\n",
    "# Destination parent folder\n",
    "dest_parent = Path(r\"C:\\Users\\MICILMEDS\\Documents\\Medi_final\\correct_data\\Category\\Drive_splitted\")\n",
    "dest_parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "chunk_size = 1999\n",
    "\n",
    "# Separate valid and empty files\n",
    "all_files = [f for f in source_folder.iterdir() if f.is_file()]\n",
    "valid_files = [f for f in all_files if f.stat().st_size > 0]\n",
    "empty_files = [f for f in all_files if f.stat().st_size == 0]\n",
    "\n",
    "# Remove empty files with progress bar\n",
    "removed_files = []\n",
    "for f in tqdm(empty_files, desc=\"Removing empty files\", unit=\"file\"):\n",
    "    f.unlink()\n",
    "    removed_files.append(f.name)\n",
    "\n",
    "# Split valid files into batches\n",
    "num_batches = math.ceil(len(valid_files) / chunk_size)\n",
    "copied_files = []\n",
    "\n",
    "for i in range(num_batches):\n",
    "    batch_folder = dest_parent / f\"batch_{i+1:02d}\"\n",
    "    batch_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = start_idx + chunk_size\n",
    "    batch_files = valid_files[start_idx:end_idx]\n",
    "\n",
    "    for file in tqdm(batch_files, desc=f\"Copying to {batch_folder.name}\", unit=\"file\"):\n",
    "        # Keep original file name\n",
    "        shutil.copy(file, batch_folder)\n",
    "        copied_files.append(file.name)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n✅ Process Complete\")\n",
    "print(f\"Total empty files removed: {len(removed_files)}\")\n",
    "if removed_files:\n",
    "    print(f\"Removed files: {removed_files}\")\n",
    "\n",
    "print(f\"\\nTotal valid files copied: {len(copied_files)}\")\n",
    "if copied_files:\n",
    "    print(f\"Copied files: {copied_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87233629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "folder_path = \"c:\\\\Users\\\\MICILMEDS\\\\Documents\\\\Medi_final\\\\correct_data\\\\Category\\\\All Brand\\\\Data\"\n",
    "extract_files = glob.glob(os.path.join(folder_path,\"*xlsx\"))\n",
    "df = pd.concat([pd.read_excel(file) for file in extract_files],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c0442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allbrand = pd.read_excel(\"c:\\\\Users\\\\MICILMEDS\\\\Documents\\\\Medi_final\\\\correct_data\\\\Category\\\\All Brand\\\\All_brands.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18298d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allbrand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55482222",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161cfe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22361411",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Product Name'].str.contains(\"\", case=False, na=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a22d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sum = 0\n",
    "for i in range(ord('A'),ord('Z')+1):\n",
    "    letter = chr(i)\n",
    "    data = df['Product Name'].str.lower().str.startswith(letter.lower()).sum()\n",
    "    print(f\"total product of {letter} are {data}\")\n",
    "    total_sum+=data\n",
    "print(f\"Total Data {total_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120915b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9481d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_excel(\"c:\\\\Users\\\\MICILMEDS\\\\Downloads\\\\20250930044502.xlsx\")\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77903c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=df_new.drop(['Key','key'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c6fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anti = df[~df['Product Name'].isin(df_new['Product Name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1947ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4970dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anti.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be5ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Product Name'].str.lower().str.contains(\"ciplox\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ba4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47457f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anti=df_anti.drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c574f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anti.to_excel(\"C:\\\\Users\\\\MICILMEDS\\\\Documents\\\\Medi_final\\\\correct_data\\\\Category\\\\remaining_data.xlsx\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pands",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
